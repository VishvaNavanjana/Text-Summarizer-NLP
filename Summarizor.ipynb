{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699a204c",
   "metadata": {
    "id": "699a204c"
   },
   "source": [
    "## You are provided a long text which is needed to summerize. \n",
    "### We are expecting you to do following steps. \n",
    "\n",
    "    Word tokenization\n",
    "    Check Word frequencies\n",
    "    Text cleaning\n",
    "    Sentence Tokenization\n",
    "    Calculate Sentence scores using word frequencies\n",
    "    Summerization\n",
    "    \n",
    "Use pure NLP techniques to achive this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e0b5893",
   "metadata": {
    "id": "3e0b5893"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as \"since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well\". They can be nuanced, such as \"X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist\".\n",
    "Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.\n",
    "The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c8fa86",
   "metadata": {
    "id": "43c8fa86"
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2c605c",
   "metadata": {
    "id": "2a2c605c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5b3a0f11c828>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTOP_WORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mheapq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnlargest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Ubt77XiHSnl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Ubt77XiHSnl",
    "outputId": "7956d27e-dd3e-4574-aa91-08c7dee0e1f9"
   },
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#Word tokenization\n",
    "tokens = []\n",
    "\n",
    "for token in doc:\n",
    "  tokens.append(token)\n",
    "\n",
    "print(\"Number of tokens : \" + str(len(tokens)))\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0uo2xSt87PS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0uo2xSt87PS",
    "outputId": "ab2ff630-4143-4ac7-ff64-32c8143d713c"
   },
   "outputs": [],
   "source": [
    "# Check Word frequencies\n",
    "\n",
    "#remove stopwords and punctuations\n",
    "words = [token.text for token in doc]\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-HJO7LuSHcAe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HJO7LuSHcAe",
    "outputId": "bed2d379-f91e-4142-c831-fcaf383cca94"
   },
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "\n",
    "#remove stopwords and punctuations\n",
    "\n",
    "nouns = [token.text\n",
    "         for token in doc\n",
    "         if (not token.is_stop and\n",
    "             not token.is_punct and\n",
    "             token.pos_ == \"NOUN\")]\n",
    "\n",
    "\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.pos_ == \"NOUN\"]\n",
    "print (words)\n",
    "\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xu5ov4loI9Zb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xu5ov4loI9Zb",
    "outputId": "dddf73b0-5586-48f2-a65e-9403570f2223"
   },
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "sentence_tokens= [sent for sent in doc.sents]\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hC-kvW-9JbQL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hC-kvW-9JbQL",
    "outputId": "851de2bd-b6f3-4c3e-a1c4-08a222b34fde"
   },
   "outputs": [],
   "source": [
    "# Calculate Sentence scores using word frequencies\n",
    "\n",
    "stopwords=list(STOP_WORDS)\n",
    "from string import punctuation\n",
    "punctuation=punctuation+ '\\n'\n",
    "\n",
    "\n",
    "word_frequencies={}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in sentence_scores.keys():                            \n",
    "             sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "             sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "\n",
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Go5y7TgTL2rl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go5y7TgTL2rl",
    "outputId": "5cae1e4e-2faa-4688-a4f9-3b6715577c5d"
   },
   "outputs": [],
   "source": [
    "# Summerization\n",
    "\n",
    "select_length=int(len(sentence_tokens)*0.3)\n",
    "select_length\n",
    "summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0JhivREdMKGd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "0JhivREdMKGd",
    "outputId": "8cbb58f4-ca22-451c-f964-384aa696b88b"
   },
   "outputs": [],
   "source": [
    "# Get the summary of text.\n",
    "\n",
    "final_summary=[word.text for word in summary]\n",
    "final_summary\n",
    "summary=''.join(final_summary)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-XqrYUR_MQVU",
   "metadata": {
    "id": "-XqrYUR_MQVU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
